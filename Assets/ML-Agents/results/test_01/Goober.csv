Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,2.7587063,99.0,-0.07439635,-2.2191919365314523,-2.2191919365314523,1.0
20000,2.7568007,99.0,-0.27007422,-1.8729999959468842,-1.8729999959468842,1.0
30000,2.7557912,99.0,-0.2923022,-2.212999990582466,-2.212999990582466,1.0
40000,2.7549489,99.0,-0.41614354,-1.6549999880790711,-1.6549999880790711,1.0
50000,2.7548494,99.0,-0.4300021,-2.637999955415726,-2.637999955415726,1.0
60000,2.753215,99.0,-0.45414242,-2.025000004172325,-2.025000004172325,1.0
70000,2.7433434,99.0,-0.5370121,-1.5209999942779542,-1.5209999942779542,1.0
80000,2.7302516,99.0,-0.53329873,-1.814999966174364,-1.814999966174364,1.0
90000,2.730795,99.0,-0.59855306,-1.9960000143945218,-1.9960000143945218,1.0
100000,2.734402,99.0,-0.65125537,-2.2079999792575835,-2.2079999792575835,1.0
110000,2.732481,99.0,-0.66626054,-1.7699999976158143,-1.7699999976158143,1.0
120000,2.7257254,99.0,-0.75718474,-1.7939999902248382,-1.7939999902248382,1.0
130000,2.723883,99.0,-0.8338787,-2.239000017642975,-2.239000017642975,1.0
140000,2.7080498,99.0,-0.8836109,-2.2179999911785124,-2.2179999911785124,1.0
